{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715e8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from loans import LoanData, PredictionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5998a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILLING IN MISSING DATA AS FOLLOWS:\n",
      "Missing data in ApplicantIncome replaced with 3812.5\n",
      "Missing data in CoapplicantIncome replaced with 1188.5\n",
      "Missing data in LoanAmount replaced with 128.0\n",
      "Missing data in LoanAmountTerm replaced with 360.0\n",
      "Missing data in Dependents replaced with 0\n",
      "Missing data in SelfEmployed replaced with No\n",
      "Missing data in CreditHistory replaced with 0\n",
      "Missing data in Gender replaced with Unknown\n",
      "Missing data in Married replaced with Unknown\n",
      "\n",
      "REMOVING OUTLIERS:\n",
      "Initial range of ApplicantIncome: 150.0 - 81000.0\n",
      "Initial range of CoapplicantIncome: 0.0 - 41667.0\n",
      "Initial range of LoanAmount: 9.0 - 700.0\n",
      "Filtered range of ApplicantIncome: 150.0 - 10139.0\n",
      "Filtered range of CoapplicantIncome: 0.0 - 5701.0\n",
      "Filtered range of LoanAmount: 9.0 - 260.0\n"
     ]
    }
   ],
   "source": [
    "# Load and clean the data\n",
    "data_full = LoanData('../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb63adfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Gender_Unknown</th>\n",
       "      <th>Married_No</th>\n",
       "      <th>Married_Unknown</th>\n",
       "      <th>Married_Yes</th>\n",
       "      <th>Rural</th>\n",
       "      <th>Semiurban</th>\n",
       "      <th>Urban</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Graduate</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>LoanAmountTerm</th>\n",
       "      <th>CreditHistory</th>\n",
       "      <th>LoanStatus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LP001570</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4167.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP001263</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3167.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP002898</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP001241</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP002625</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3583.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP001491</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3316.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP002530</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2873.0</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP001245</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP002237</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3667.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LP002862</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6125.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Gender_Female  Gender_Male  Gender_Unknown  Married_No  \\\n",
       "LoanID                                                             \n",
       "LP001570              0            1               0           0   \n",
       "LP001263              0            1               0           0   \n",
       "LP002898              0            1               0           0   \n",
       "LP001241              1            0               0           1   \n",
       "LP002625              0            0               1           1   \n",
       "...                 ...          ...             ...         ...   \n",
       "LP001491              0            1               0           0   \n",
       "LP002530              0            0               1           0   \n",
       "LP001245              0            1               0           0   \n",
       "LP002237              0            1               0           1   \n",
       "LP002862              0            1               0           0   \n",
       "\n",
       "          Married_Unknown  Married_Yes  Rural  Semiurban  Urban  Dependents  \\\n",
       "LoanID                                                                        \n",
       "LP001570                0            1      1          0      0           2   \n",
       "LP001263                0            1      0          1      0           3   \n",
       "LP002898                0            1      1          0      0           1   \n",
       "LP001241                0            0      0          1      0           0   \n",
       "LP002625                0            0      0          0      1           0   \n",
       "...                   ...          ...    ...        ...    ...         ...   \n",
       "LP001491                0            1      0          0      1           2   \n",
       "LP002530                0            1      0          1      0           2   \n",
       "LP001245                0            1      0          1      0           2   \n",
       "LP002237                0            0      0          0      1           1   \n",
       "LP002862                0            1      0          1      0           2   \n",
       "\n",
       "          Graduate  SelfEmployed  ApplicantIncome  CoapplicantIncome  \\\n",
       "LoanID                                                                 \n",
       "LP001570         1             0           4167.0             1447.0   \n",
       "LP001263         1             0           3167.0             4000.0   \n",
       "LP002898         1             0           1880.0                0.0   \n",
       "LP001241         1             0           4300.0                0.0   \n",
       "LP002625         1             0           3583.0                0.0   \n",
       "...            ...           ...              ...                ...   \n",
       "LP001491         1             1           3316.0             3500.0   \n",
       "LP002530         1             0           2873.0             1872.0   \n",
       "LP001245         0             1           1875.0             1875.0   \n",
       "LP002237         1             0           3667.0                0.0   \n",
       "LP002862         0             0           6125.0             1625.0   \n",
       "\n",
       "          LoanAmount  LoanAmountTerm  CreditHistory  LoanStatus  \n",
       "LoanID                                                           \n",
       "LP001570       158.0           360.0            1.0           1  \n",
       "LP001263       180.0           300.0            0.0           0  \n",
       "LP002898        61.0           360.0            0.0           0  \n",
       "LP001241       136.0           360.0            0.0           0  \n",
       "LP002625        96.0           360.0            1.0           0  \n",
       "...              ...             ...            ...         ...  \n",
       "LP001491        88.0           360.0            1.0           1  \n",
       "LP002530       132.0           360.0            0.0           0  \n",
       "LP001245        97.0           360.0            1.0           1  \n",
       "LP002237       113.0           180.0            1.0           1  \n",
       "LP002862       187.0           480.0            1.0           0  \n",
       "\n",
       "[326 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a subset of the data that is split 50:50 between approved and unapproved loans for a more balanced dataset\n",
    "approved = data_full.cleaned[data_full.cleaned.LoanStatus == 1]\n",
    "unapproved = data_full.cleaned[data_full.cleaned.LoanStatus == 0]\n",
    "data_even = pd.concat([approved.sample(len(unapproved)),unapproved]).sample(frac=1)\n",
    "data_even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d083cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up two pipelines, one for each dataset\n",
    "pipe_on_full, pipe_on_even = PredictionPipeline(), PredictionPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb393fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scalers for each dataset\n",
    "pipe_on_full.fit_scaler(data_full)\n",
    "pipe_on_even.fit_scaler(data_even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aff0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "120 fits failed out of a total of 1080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 462, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 512, in _fit\n",
      "    return self._fit_full(X, n_components)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 526, in _fit_full\n",
      "    raise ValueError(\n",
      "ValueError: n_components=18 must be between 0 and min(n_samples, n_features)=17 with svd_solver='full'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 462, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 512, in _fit\n",
      "    return self._fit_full(X, n_components)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 526, in _fit_full\n",
      "    raise ValueError(\n",
      "ValueError: n_components=19 must be between 0 and min(n_samples, n_features)=17 with svd_solver='full'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.69533195 0.69533195 0.69530057 0.70277028 0.7644948  0.7644948\n",
      " 0.7644948  0.7644948  0.7644948  0.7644948  0.7644948  0.7644948\n",
      " 0.7644948  0.7644948  0.7644948  0.7644948         nan        nan\n",
      " 0.69533195 0.69720461 0.68967213 0.7083778  0.76262214 0.76822966\n",
      " 0.76448434 0.766357   0.766357   0.77010232 0.77010232 0.77010232\n",
      " 0.77010232 0.77010232 0.77010232 0.77010232        nan        nan\n",
      " 0.69533195 0.69720461 0.68780993 0.71398531 0.77196451 0.76824012\n",
      " 0.76824012 0.77010232 0.77010232 0.76824012 0.76824012 0.76824012\n",
      " 0.76824012 0.76824012 0.76824012 0.76824012        nan        nan\n",
      " 0.69533195 0.69907727 0.68968259 0.7252108  0.76824012 0.76824012\n",
      " 0.77383717 0.77010232 0.76822966 0.77010232 0.77197498 0.77197498\n",
      " 0.77197498 0.77197498 0.77197498 0.77197498        nan        nan\n",
      " 0.69533195 0.69907727 0.69342791 0.72334861 0.77196451 0.76824012\n",
      " 0.77382671 0.77196451 0.77196451 0.77010232 0.77010232 0.77197498\n",
      " 0.77197498 0.77197498 0.77197498 0.77197498        nan        nan\n",
      " 0.69533195 0.69907727 0.69342791 0.72895612 0.77196451 0.76824012\n",
      " 0.77569937 0.77196451 0.77009185 0.77010232 0.77197498 0.77010232\n",
      " 0.77010232 0.77010232 0.77010232 0.77010232        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72709393 0.77196451 0.77011278\n",
      " 0.77569937 0.77009185 0.77009185 0.77010232 0.77197498 0.77197498\n",
      " 0.77197498 0.77197498 0.77197498 0.77197498        nan        nan\n",
      " 0.69533195 0.69907727 0.69342791 0.73082878 0.77196451 0.77011278\n",
      " 0.77570983 0.77196451 0.77009185 0.76822966 0.77010232 0.77010232\n",
      " 0.77009185 0.77009185 0.77009185 0.77009185        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72896659 0.77196451 0.77011278\n",
      " 0.77569937 0.77009185 0.77009185 0.77010232 0.77197498 0.77010232\n",
      " 0.77010232 0.77010232 0.77010232 0.77010232        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72896659 0.77196451 0.76824012\n",
      " 0.77570983 0.77196451 0.7682192  0.76822966 0.76822966 0.77009185\n",
      " 0.7682192  0.7682192  0.7682192  0.7682192         nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72896659 0.77196451 0.77011278\n",
      " 0.77569937 0.77009185 0.77009185 0.76822966 0.76822966 0.77196451\n",
      " 0.77009185 0.77009185 0.77009185 0.77009185        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72896659 0.77196451 0.76824012\n",
      " 0.77570983 0.77196451 0.7682192  0.76822966 0.76822966 0.7682192\n",
      " 0.7682192  0.7682192  0.7682192  0.7682192         nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72709393 0.77196451 0.76824012\n",
      " 0.77382671 0.77009185 0.77009185 0.76822966 0.76822966 0.77196451\n",
      " 0.7682192  0.7682192  0.7682192  0.7682192         nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72709393 0.77196451 0.76824012\n",
      " 0.77383717 0.77196451 0.7682192  0.76822966 0.77009185 0.7682192\n",
      " 0.76634654 0.76634654 0.76634654 0.76634654        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72709393 0.77196451 0.76824012\n",
      " 0.77382671 0.77196451 0.7682192  0.76822966 0.77010232 0.77196451\n",
      " 0.76634654 0.76634654 0.76634654 0.76634654        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72709393 0.77196451 0.76824012\n",
      " 0.77383717 0.77196451 0.7682192  0.76822966 0.77196451 0.77009185\n",
      " 0.76634654 0.76634654 0.76634654 0.76634654        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72522127 0.77196451 0.76824012\n",
      " 0.77382671 0.77196451 0.7682192  0.76822966 0.77010232 0.7682192\n",
      " 0.76447388 0.76447388 0.76447388 0.76447388        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72522127 0.77196451 0.76824012\n",
      " 0.77383717 0.77196451 0.7682192  0.76822966 0.77196451 0.77009185\n",
      " 0.76634654 0.76634654 0.76634654 0.76634654        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72522127 0.77196451 0.76824012\n",
      " 0.77382671 0.77196451 0.7682192  0.76822966 0.77196451 0.7682192\n",
      " 0.76447388 0.76447388 0.76447388 0.76447388        nan        nan\n",
      " 0.69533195 0.69907727 0.69530057 0.72522127 0.77196451 0.76824012\n",
      " 0.77383717 0.77196451 0.7682192  0.76822966 0.77196451 0.7682192\n",
      " 0.76634654 0.76634654 0.76634654 0.76634654        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr__C': 1.7, 'lr__penalty': 'l2', 'lr__solver': 'liblinear', 'pca__n_components': 8}\n",
      "Accuracy score: 0.7757098319837633\n",
      "Do you wish to implement this model? (y/n):y\n",
      "Model has been fit and implemented.\n"
     ]
    }
   ],
   "source": [
    "# Test and fit the logistic regression for the full dataset\n",
    "pipe_on_full.test_and_fit_estimator(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db0f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "120 fits failed out of a total of 1080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 462, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 512, in _fit\n",
      "    return self._fit_full(X, n_components)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 526, in _fit_full\n",
      "    raise ValueError(\n",
      "ValueError: n_components=18 must be between 0 and min(n_samples, n_features)=17 with svd_solver='full'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 462, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 512, in _fit\n",
      "    return self._fit_full(X, n_components)\n",
      "  File \"C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 526, in _fit_full\n",
      "    raise ValueError(\n",
      "ValueError: n_components=19 must be between 0 and min(n_samples, n_features)=17 with svd_solver='full'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Stata\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.60743006 0.58908144 0.6566712  0.68725224 0.68725224 0.68725224\n",
      " 0.68725224 0.68725224 0.68725224 0.68725224 0.68725224 0.68725224\n",
      " 0.68725224 0.68725224 0.68725224 0.68725224        nan        nan\n",
      " 0.5890248  0.60131385 0.66281572 0.6688753  0.67499151 0.69033866\n",
      " 0.70562918 0.71788991 0.7148318  0.7148318  0.7087156  0.7117737\n",
      " 0.7087156  0.7087156  0.7087156  0.7087156         nan        nan\n",
      " 0.60743006 0.58908144 0.6566712  0.69033866 0.69648318 0.7117737\n",
      " 0.7148318  0.71788991 0.71788991 0.71788991 0.7148318  0.7148318\n",
      " 0.7148318  0.7148318  0.7148318  0.7148318         nan        nan\n",
      " 0.59514101 0.60131385 0.65975762 0.6719334  0.68416582 0.70874391\n",
      " 0.70568581 0.72709254 0.7148318  0.71788991 0.7148318  0.7148318\n",
      " 0.7117737  0.7117737  0.7117737  0.7117737         nan        nan\n",
      " 0.60743006 0.58908144 0.6566712  0.69339676 0.67807793 0.72400612\n",
      " 0.71788991 0.71788991 0.70562918 0.70562918 0.70562918 0.70562918\n",
      " 0.70562918 0.70562918 0.70562918 0.70562918        nan        nan\n",
      " 0.59514101 0.60131385 0.65055499 0.67196172 0.68722392 0.71180202\n",
      " 0.70568581 0.73015064 0.7117737  0.71788991 0.7148318  0.71788991\n",
      " 0.71788991 0.71788991 0.71788991 0.71788991        nan        nan\n",
      " 0.60437196 0.58602333 0.65361309 0.69033866 0.68419413 0.71791822\n",
      " 0.71180202 0.72709254 0.7117737  0.7117737  0.7148318  0.71174538\n",
      " 0.71174538 0.71174538 0.71174538 0.71174538        nan        nan\n",
      " 0.59819912 0.60131385 0.65055499 0.67807793 0.68722392 0.7087156\n",
      " 0.70568581 0.73015064 0.70868728 0.72094801 0.7148318  0.71480349\n",
      " 0.71786159 0.71786159 0.71786159 0.71786159        nan        nan\n",
      " 0.60128554 0.58908144 0.65361309 0.68110771 0.68419413 0.71791822\n",
      " 0.70568581 0.73015064 0.7117737  0.7117737  0.7148318  0.70865896\n",
      " 0.70865896 0.70865896 0.70865896 0.70865896        nan        nan\n",
      " 0.59819912 0.60131385 0.64749689 0.67807793 0.68416582 0.7087156\n",
      " 0.70568581 0.72709254 0.71174538 0.72094801 0.72097633 0.71480349\n",
      " 0.71174538 0.71174538 0.71174538 0.71174538        nan        nan\n",
      " 0.60128554 0.59519764 0.65361309 0.68110771 0.68419413 0.71791822\n",
      " 0.70568581 0.73015064 0.7117737  0.7148318  0.71788991 0.71171707\n",
      " 0.71171707 0.71171707 0.71171707 0.71171707        nan        nan\n",
      " 0.59819912 0.60131385 0.65055499 0.67807793 0.68725224 0.7087156\n",
      " 0.70568581 0.72709254 0.70868728 0.7148318  0.72403443 0.71174538\n",
      " 0.71480349 0.71480349 0.71480349 0.71480349        nan        nan\n",
      " 0.59822743 0.59519764 0.65361309 0.67804961 0.68725224 0.71486012\n",
      " 0.7026277  0.72709254 0.7148318  0.7148318  0.71788991 0.71171707\n",
      " 0.70865896 0.70865896 0.70865896 0.70865896        nan        nan\n",
      " 0.59514101 0.60131385 0.65055499 0.67807793 0.68725224 0.7087156\n",
      " 0.70568581 0.72709254 0.70868728 0.7148318  0.72097633 0.71480349\n",
      " 0.71480349 0.71480349 0.71480349 0.71480349        nan        nan\n",
      " 0.59822743 0.59519764 0.65361309 0.67804961 0.68725224 0.71486012\n",
      " 0.70568581 0.72709254 0.7148318  0.7148318  0.71788991 0.71171707\n",
      " 0.70865896 0.70865896 0.70865896 0.70865896        nan        nan\n",
      " 0.59514101 0.60131385 0.65055499 0.67807793 0.68725224 0.7087156\n",
      " 0.70568581 0.72709254 0.70868728 0.7117737  0.72097633 0.71480349\n",
      " 0.71480349 0.71480349 0.71480349 0.71480349        nan        nan\n",
      " 0.59822743 0.59519764 0.65361309 0.67804961 0.68416582 0.71486012\n",
      " 0.70568581 0.72709254 0.71174538 0.71788991 0.71788991 0.71171707\n",
      " 0.70560086 0.70560086 0.70560086 0.70560086        nan        nan\n",
      " 0.59514101 0.59825575 0.65055499 0.67807793 0.68725224 0.7087156\n",
      " 0.70568581 0.72709254 0.70868728 0.7148318  0.72097633 0.71174538\n",
      " 0.71174538 0.71174538 0.71174538 0.71174538        nan        nan\n",
      " 0.59514101 0.59519764 0.65361309 0.67804961 0.68416582 0.71486012\n",
      " 0.70568581 0.72709254 0.71174538 0.71788991 0.71788991 0.70865896\n",
      " 0.70560086 0.70560086 0.70560086 0.70560086        nan        nan\n",
      " 0.59514101 0.59825575 0.65055499 0.67807793 0.68725224 0.7087156\n",
      " 0.70568581 0.72709254 0.70868728 0.7148318  0.72097633 0.71174538\n",
      " 0.71174538 0.71174538 0.71174538 0.71174538        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr__C': 1.2, 'lr__penalty': 'l2', 'lr__solver': 'liblinear', 'pca__n_components': 9}\n",
      "Accuracy score: 0.7301506399365726\n",
      "Do you wish to implement this model? (y/n):y\n",
      "Model has been fit and implemented.\n"
     ]
    }
   ],
   "source": [
    "# Test and fit the logistic regression for the full dataset\n",
    "pipe_on_even.test_and_fit_estimator(data_even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b228f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check predictions for all four methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ada4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (overall): 77.76%\n",
      "Accuracy (y=0): 49.08%\n",
      "Accuracy (y=1): 90.32%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_on_full.predict(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb9e8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (overall): 74.58%\n",
      "Accuracy (y=0): 66.87%\n",
      "Accuracy (y=1): 77.96%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_on_even.predict(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c4b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store both the models\n",
    "pipe_on_full.save('PipeOnFull')\n",
    "pipe_on_even.save('PipeOnEven')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28d25a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_full = PredictionPipeline()\n",
    "new_full.load('PipeOnFull')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
