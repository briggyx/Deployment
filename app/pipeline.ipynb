{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoanData:\n",
    "    \n",
    "    def __init__(self,csv_path,do_cleaning=True):\n",
    "        \"\"\"Initialize a new LoanPipeline, loading data from the specified CSV file\"\"\"\n",
    "        self.original = pd.read_csv(csv_path)\n",
    "        if do_cleaning:\n",
    "            self.clean()\n",
    "        else:\n",
    "            self.cleaned = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Equivalent to the most complete internal dataframe's __repr__\"\"\"\n",
    "\n",
    "        if self.cleaned is not None:\n",
    "            print('CLEANED DATA:')\n",
    "            display(self.cleaned)\n",
    "        else:\n",
    "            print('ORIGINAL DATA:')\n",
    "            display(self.original)\n",
    "        return('')\n",
    "        \n",
    "    def to_csv(self,csv_path,which='original'):\n",
    "        \"\"\"Write the data to a CSV; which=['original','cleaned']\"\"\"\n",
    "        if which == 'original':\n",
    "            self.original.to_csv(csv_path,index=False)\n",
    "        elif which == 'cleaned':\n",
    "            if self.cleaned is None:\n",
    "                print('Error: no cleaned data to write to CSV.')\n",
    "                return\n",
    "            self.cleaned.to_csv(csv_path,index=False)\n",
    "        else:\n",
    "            print(f'Invalid value for which parameter: {which}')\n",
    "    \n",
    "    def clean(self):\n",
    "        \"\"\"Tidy column names, fill in missing data, convert data types\"\"\"\n",
    "        \n",
    "        # Remove trailing whitespace from strings\n",
    "        self.original = self.original.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        \n",
    "        # Remove underscores from column names for consistency\n",
    "        self.original = self.original.rename(columns=lambda x: x.replace('_', ''))\n",
    "        \n",
    "        # Make index = loan ID, and remove loan ID column\n",
    "        self.cleaned = self.original.copy()\n",
    "        self.cleaned.index = self.cleaned.LoanID\n",
    "        self.cleaned = self.cleaned[self.cleaned.columns[1:]]\n",
    "        \n",
    "        # Fill missing data with median, mode, or zero, as appropriate\n",
    "        medians = self.cleaned[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'LoanAmountTerm']].median() # Calculate medians\n",
    "        modes = self.cleaned[['Dependents','SelfEmployed']].mode() # Calculate modes\n",
    "        replace = { x:medians[x] for x in medians.index } # Dict to hold column name to value to sub in place of nan mapping\n",
    "        replace['Dependents'] = modes.Dependents.sample(1)[0] # Sample in case mode returned multiple values\n",
    "        replace['SelfEmployed'] = modes.SelfEmployed.sample(1)[0] # Sample in case mode returned multiple values\n",
    "        replace['CreditHistory'] = 0 # Assume missing values in CreditHistory mean no history is available, = 0\n",
    "        replace['Gender'] = 'Unknown'\n",
    "        replace['Married'] = 'Unknown'\n",
    "        print('FILLING IN MISSING DATA AS FOLLOWS:')\n",
    "        for col,val in replace.items():\n",
    "            print(f'Missing data in {col} replaced with {val}')\n",
    "        self.cleaned.fillna(replace,inplace=True) # Fill nans with medians\n",
    "        \n",
    "        # Convert numeric data to float32; some would be fine as integers due to lack of decimals but new data may break that assumption\n",
    "        self.cleaned.ApplicantIncome = self.cleaned.ApplicantIncome.astype(np.float32)\n",
    "        self.cleaned.CoapplicantIncome = self.cleaned.CoapplicantIncome.astype(np.float32)\n",
    "        self.cleaned.LoanAmount = self.cleaned.LoanAmount.astype(np.float32)\n",
    "        self.cleaned.LoanAmountTerm = self.cleaned.LoanAmount.astype(np.float32)\n",
    "        \n",
    "        # Convert ordered categorical data to unsigned 8-bit integers\n",
    "        self.cleaned.Dependents = self.cleaned.Dependents.map({'0':0,'1':1,'2':2,'3+':3}).astype(np.uint8)\n",
    "        self.cleaned.SelfEmployed = self.cleaned.SelfEmployed.map({'Yes': 1, 'No': 0}).astype(np.uint8)\n",
    "        self.cleaned.LoanStatus = self.cleaned.LoanStatus.map({'Y': 1, 'N': 0}).astype(np.uint8)\n",
    "        self.cleaned.Education = self.cleaned.Education.map({'Graduate': 1, 'Not Graduate': 0}).astype(np.uint8)\n",
    "        self.cleaned.rename(columns={'Education':'Graduate'},inplace=True)\n",
    "        \n",
    "        # One-hot-encode gender and marriage status (necessary because missing data was treated as 'other')\n",
    "        gender = pd.get_dummies(self.cleaned.Gender)\n",
    "        marriage = pd.get_dummies(self.cleaned.Married)\n",
    "        gender.columns = [ f'Gender_{g}' for g in gender.columns ]\n",
    "        marriage.columns = [ f'Married_{m}' for m in marriage.columns ]\n",
    "        prop = pd.get_dummies(self.cleaned.PropertyArea)\n",
    "        one_hot = pd.merge(pd.merge(gender,marriage,left_index=True,right_index=True),prop,left_index=True,right_index=True)\n",
    "        \n",
    "        # Merge everything\n",
    "        self.cleaned = pd.merge(one_hot,self.cleaned[self.cleaned.columns[2:].drop('PropertyArea')],left_index=True,right_index=True)\n",
    "    \n",
    "        # Remove outliers\n",
    "        print('\\nREMOVING OUTLIERS:')\n",
    "        cols_to_check = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
    "        \n",
    "        for col in cols_to_check:\n",
    "            print(f\"Initial range of {col}: {self.cleaned[col].min()} - {self.cleaned[col].max()}\")\n",
    "        \n",
    "        q1, q4 = self.cleaned[cols_to_check].quantile(0.25), self.cleaned[cols_to_check].quantile(0.75)\n",
    "        iqr = q4 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q4 + 1.5 * iqr\n",
    "        \n",
    "        mask = (self.cleaned[cols_to_check] >= lower_bound) & (self.cleaned[cols_to_check] <= upper_bound)\n",
    "        self.cleaned = self.cleaned[mask.all(axis=1)].copy()\n",
    "        \n",
    "        for col in cols_to_check:\n",
    "            print(f\"Filtered range of {col}: {self.cleaned[col].min()} - {self.cleaned[col].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionPipeline:\n",
    "    \"\"\"Implements data cleaning and loan prediction functionality; initialize with a path to a CSV file\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = None\n",
    "        self.estimator = None\n",
    "        self.neural = None\n",
    "    \n",
    "    def _check_data_(self,data):\n",
    "        if type(data) == LoanData:\n",
    "            if data.cleaned is not None:\n",
    "                data = data.cleaned\n",
    "            else:\n",
    "                print('Error: LoanData object passed to fit_scaler, but no cleaned data found; run LoanData.clean() first')\n",
    "                return\n",
    "        return data\n",
    "    \n",
    "    def _get_Xy_(self,data,label_target):\n",
    "        data = pd.DataFrame(self.scaler.transform(data),columns=data.columns)\n",
    "        X = data.drop(columns=[label_target])\n",
    "        y = data[label_target] if label_target in data.columns else None\n",
    "        return X,y\n",
    "    \n",
    "    def fit_scaler(self,data,scaler = MinMaxScaler()):\n",
    "        \"\"\"Fit a scaler, default = min-max\"\"\"\n",
    "        \n",
    "        data = self._check_data_(data)\n",
    "        scaler.fit_transform(data)\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def test_and_fit_estimator(self,data,estimator,\n",
    "                               label_target='LoanStatus',\n",
    "                               ncomp=np.arange(2,20,1),\n",
    "                               penalties=[\"l1\", \"l2\"],\n",
    "                               C=np.arange(1,25,2.5)/5,\n",
    "                               solvers=['liblinear']):\n",
    "        \"\"\"Takes an estimator and uses grid search to find the best parameters, and reports these along with model accuracy, prompting the user to accept or reject the model\"\"\"\n",
    "        \n",
    "        data = self._check_data_(data)\n",
    "        \n",
    "        if scaler is None:\n",
    "            print('Error: No scaler implemented yet; run PredictionPipeline.fit_scaler() first.')\n",
    "            return\n",
    "        \n",
    "        X,y = self._get_Xy_(data,label_target)\n",
    "        \n",
    "        param_grid = { \"pca__n_components\": ncomp, \"lr__penalty\": penalties,\n",
    "        \"lr__C\": C, \"lr__solver\": solvers }\n",
    "\n",
    "        # Combine PCA and logistic regression into a pipeline\n",
    "        pipe = Pipeline([(\"pca\", PCA()),(\"lr\", estimator)])\n",
    "\n",
    "        # Create a grid search object\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=3)\n",
    "        \n",
    "        # Fit the grid search object to the data\n",
    "        grid.fit(X, y)\n",
    "\n",
    "        # Print the best hyperparameters and accuracy score\n",
    "        print(\"Best hyperparameters:\", grid.best_params_)\n",
    "        print(\"Accuracy score:\", grid.best_score_)\n",
    "        \n",
    "        # Prompt the user to accept or reject the model\n",
    "        accept = input('Do you wish to implement this model? (y/n):')\n",
    "        if accept.upper() != 'Y':\n",
    "            print('Model not implemented; please run PredictionPipeline.test_and_fit() again')\n",
    "            return\n",
    "        \n",
    "        # If the model is good, fit and store it\n",
    "        self.estimator = grid.best_estimator_\n",
    "        self.estimator.fit(X,y)\n",
    "        print('Model has been fit and implemented.')\n",
    "    \n",
    "    def train_neural(self,data,\n",
    "                     label_target='LoanStatus',\n",
    "                     layers=[30,20,10],\n",
    "                     dropout=0.25,\n",
    "                     optimizer='adam',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'],\n",
    "                     epochs=500,\n",
    "                     val_frac=0.35):\n",
    "        \n",
    "        data = self._check_data_(data)\n",
    "        \n",
    "        if scaler is None:\n",
    "            print('Error: No scaler implemented yet; run PredictionPipeline.fit_scaler() first.')\n",
    "            return\n",
    "        \n",
    "        tensor = X.to_numpy(dtype='float32')\n",
    "        tensor_labels = y.to_numpy(dtype='float32')\n",
    "        \n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(layers[0],input_dim=tensor.shape[1],activation='relu'))\n",
    "        if dropout:\n",
    "            model.add(keras.layers.Dropout(dropout))\n",
    "        for n in layers[1:]:\n",
    "            model.add(keras.layers.Dense(n,activation='relu'))\n",
    "            if dropout:\n",
    "                model.add(keras.layers.Dropout(dropout))\n",
    "        model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "        \n",
    "        split = int(round(tensor.shape[0]*val_frac))\n",
    "        \n",
    "        # Checkpoint for saving the best model, despite over-fitting\n",
    "        checkpoint = ModelCheckpoint('best_model.h5',monitor='val_accuracy', save_best_only=True, mode='max', verbose=0)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(tensor[split:,:], tensor_labels[split:],\n",
    "                            validation_data=(tensor[:split,:], tensor_labels[:split]),\n",
    "                            epochs=epochs,batch_size=split,verbose=0,callbacks=[checkpoint])\n",
    "        \n",
    "        # Load the weights from the best model\n",
    "        model.load_weights('best_model.h5')\n",
    "        \n",
    "        # Plot the training process\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print the validation accuracy of the best model\n",
    "        best_val_acc = max(history.history['val_accuracy'])\n",
    "        print('Validation accuracy of the best model:', best_val_acc)\n",
    "        \n",
    "        # Prompt the user to accept/reject\n",
    "        accept = input('Accept this model? (y/n):')\n",
    "        \n",
    "        # Implement or do not implement as specified\n",
    "        if accept.upper() != 'Y':\n",
    "            print('Model not implemented; run PredictionPipeline.neural() again.')\n",
    "            return\n",
    "        self.neural = model\n",
    "        print('Model has been fit and implemented.')\n",
    "    \n",
    "    def _convert_web_input_(json):\n",
    "        # Take parsed json data, convert it to one-row DF that matches cleaned data\n",
    "        pass\n",
    "    \n",
    "    def predict(self,data,neural=False,label_target='LoanStatus'):\n",
    "        \"\"\"Run predictions; if y is supplied, accuracy is checked\"\"\"\n",
    "        \n",
    "        data = self._check_data_(data)\n",
    "        \n",
    "        if scaler is None:\n",
    "            print('Error: No scaler implemented yet; run PredictionPipeline.fit_scaler() first.')\n",
    "            return\n",
    "        \n",
    "        X,y = self._get_Xy_(data,label_target)\n",
    "        \n",
    "        if neural:\n",
    "            \n",
    "            if self.neural is None:\n",
    "                print('Error, prediction with neural network requested but no network is implemented; run PredictionPipeline.train_neural() first.')\n",
    "                return\n",
    "            \n",
    "            X = X.to_numpy(dtype='float32')\n",
    "            \n",
    "            predictions = self.neural.predict(X)\n",
    "            predictions = (predictions > 0.5).astype(int).T[0]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.estimator is None:\n",
    "                print('Error, no estimator is implemented; run PredictionPipeline.test_and_fit() first.')\n",
    "                return\n",
    "            \n",
    "            predictions = self.estimator.predict(X)\n",
    "            \n",
    "        if y is not None:\n",
    "                \n",
    "            predictions_0 = predictions[y==0]\n",
    "            predictions_1 = predictions[y==1]\n",
    "\n",
    "            y_0 = y[y==0]\n",
    "            y_1 = y[y==1]\n",
    "\n",
    "            accuracy_total = sum(predictions == y)/len(predictions)\n",
    "            accuracy_0 = sum(predictions_0 == y_0)/len(predictions_0)\n",
    "            accuracy_1 = sum(predictions_1 == y_1)/len(predictions_1)\n",
    "            \n",
    "            print(f'Accuracy (overall): {round(accuracy_total*100,2)}%')\n",
    "            print(f'Accuracy (y=0): {round(accuracy_0*100,2)}%')\n",
    "            print(f'Accuracy (y=1): {round(accuracy_1*100,2)}%')\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler,open('TEST.pk','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('TEST.pk','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
